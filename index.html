<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Deformable Neural Radiance Fields creates free-viewpoint portraits (nerfies) from casually captured videos.">
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Nerfies: Deformable Neural Radiance Fields</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">


  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Create Your World: Lifelong Text-to-Image Diffusion</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a >Gan Sun</a><sup>1</sup>,</span>
            <span class="author-block">
              <a >Wenqi Liang</a><sup>1</sup>,</span>
            <span class="author-block">
              <a >Jiahua Dong</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a >Jun Li</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a >Zhengming Ding</a><sup>3</sup>,
            </span>
            <span class="author-block">
              <a >Yang Cong</a><sup>4</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Shenyang Institute of Automation,</span>
            <span class="author-block"><sup>2</sup>Nanjing University of Science and Technology,</span>
            <span class="author-block"><sup>2</sup>Tulane University,</span>
            <span class="author-block"><sup>2</sup>South China University of Technology</span>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>




<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Text-to-image generative models can produce diverse high-quality images of concepts with a text prompt, which have
            demonstrated excellent ability in image generation, image translation, etc. We in this work study the problem of synthesizing instantiations
            of a user’s own concepts in a never-ending manner, i.e., create your world, where the new concepts from user are quickly learned with a
            few examples. To achieve this goal, we propose a Lifelong text-to-image Diffusion Model (L2DM), which intends to overcome knowledge
            “catastrophic forgetting” for the past encountered concepts, and semantic “catastrophic neglecting” for one or more concepts in the text
            prompt. In respect of knowledge “catastrophic forgetting”, our L2DM framework devises a task-aware memory enhancement module and
            a elastic-concept distillation module, which could respectively safeguard the knowledge of both prior concepts and each past
            personalized concept. When generating images with a user text prompt, the solution to semantic “catastrophic neglecting” is that a
            concept attention artist module can alleviate the semantic neglecting from concept aspect, and an orthogonal attention module can
            reduce the semantic binding from attribute aspect. To the end, our model can generate more faithful image across a range of continual
            text prompts in terms of both qualitative and quantitative metrics, when comparing with the related state-of-the-art models. 
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>


</body>
</html>
